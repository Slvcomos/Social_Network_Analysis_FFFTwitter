{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stackapi import StackAPI\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Preprocess meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Dati wiki/pulito_nodes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = df['Tag']\n",
    "meaning = df['Significati']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(corpus):\n",
    "    clear_text = []\n",
    "    for phrase in corpus:\n",
    "        # Replace URL with the tag url\n",
    "        phrase = re.sub(\"https*\\S+\", \"url\", phrase)\n",
    "        # Remove strange characters\n",
    "        # &quot => \"\"\n",
    "        # &#39 => '\n",
    "        # &amp; => &\n",
    "        # &lt; => <\n",
    "        # &gt; => >\n",
    "        phrase = re.sub(\"&quot;|\\r|\\n\\|&#39|&amp;|&lt;|&gt;\", \"\", phrase)\n",
    "\n",
    "        # Punctuaction removal\n",
    "        phrase = re.sub('[%s]' % re.escape(string.punctuation), ' ', phrase)\n",
    "        # Replace numbers with the tag number\n",
    "        phrase = re.sub(\"\\d+\", \"number \", phrase)\n",
    "        # Convert to lowercase\n",
    "        phrase = phrase.lower()\n",
    "        # Replace the over spaces\n",
    "        phrase = re.sub('\\s{2,}', \" \", phrase)\n",
    "        clear_text.append([phrase])\n",
    "    return clear_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_text = clean_text(meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_graphml(\"../DataSet FFF/Graph_data/Real_Network.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')\n",
    "sorted_degree = sorted(degree_dict.items(), key = itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n"
     ]
    }
   ],
   "source": [
    "sample_nodes_597 = []\n",
    "for i in sorted_degree:\n",
    "    if i[1] > 50:\n",
    "        sample_nodes_597.append(i[0])\n",
    "print(len(sample_nodes_597))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sample_meaning(sample, meaning):\n",
    "    sampled_meaning = []\n",
    "    sample_nodes = []\n",
    "    for i in sample:\n",
    "        for j in range(len(tags)):\n",
    "            if tags[j]==i:\n",
    "                sampled_meaning.append(i + \" \" + meaning[j][0])\n",
    "                sample_nodes.append(i)\n",
    "    return sampled_meaning, sample_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_meaning_597, sample_nodes_597 = select_sample_meaning(sample_nodes_597, cleared_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(sample_nodes_1057)):\\n    if not isNaN(tags[i]):\\n        sampled_meaning_1057[i] = sample_nodes_1057[i] + \" \" + sampled_meaning_1057[i][0]\\n    else:\\n        sampled_meaning_1057[i] = \"nan \" + sampled_meaning_1057[i][0]\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i in range(len(sample_nodes_1057)):\n",
    "    if not isNaN(tags[i]):\n",
    "        sampled_meaning_1057[i] = sample_nodes_1057[i] + \" \" + sampled_meaning_1057[i][0]\n",
    "    else:\n",
    "        sampled_meaning_1057[i] = \"nan \" + sampled_meaning_1057[i][0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = []\n",
    "for phrase in sampled_meaning_597:\n",
    "    word_tokens.append(phrase.split())\n",
    "stop_words = stopwords.words('english')\n",
    "no_sw_597=[]\n",
    "for phrase in word_tokens:\n",
    "    no_sw_tmp=[]\n",
    "    for word in phrase:\n",
    "        if word not in stop_words:\n",
    "            no_sw_tmp.append(word)\n",
    "    no_sw_597.append(no_sw_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_meaning_597 = []\n",
    "for i in no_sw_597:\n",
    "    new_s = \"\"\n",
    "    for j in i:\n",
    "        new_s = new_s + \" \" + j\n",
    "    cleared_meaning_597.append(new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Tags\": sample_nodes_597, \"Meaning\" : cleared_meaning_597})\n",
    "df.to_csv(\"nodes_597_meaning.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_597 = Word2Vec(no_sw_597, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2444"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec_597.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_isin(word2vec, data):\n",
    "    not_in = []\n",
    "    for i in data:\n",
    "        try:\n",
    "            word2vec.wv[i]\n",
    "        except:\n",
    "            not_in.append(i)\n",
    "    print(len(not_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Store word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkpred.evaluation import Pair\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_597 = G.subgraph(sample_nodes_597)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = []\n",
    "second = []\n",
    "for i in H_597.edges():\n",
    "    first.append(word2vec_597.wv[i[0]])\n",
    "    second.append(word2vec_597.wv[i[1]])\n",
    "new_df = pd.DataFrame({\"first\":first, \"second\":second})\n",
    "new_df.reset_index(inplace=True, drop=True)\n",
    "new_df.to_pickle(\"embedding_pickle_597.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_597.save(\"word2vec_597.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(H_597.nodes())\n",
    "universe_597 = set([Pair(i) for i in itertools.product(nodes, nodes) if i[0]!=i[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('universe_597.pickle', 'wb') as f:\n",
    "    pickle.dump(universe_597, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = []\n",
    "second = []\n",
    "for i in H_597.edges():\n",
    "    first.append(i[0])\n",
    "    second.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame({\"first\":first, \"second\":second})\n",
    "new_df.reset_index(inplace=True, drop=True)\n",
    "new_df.to_pickle(\"edges_original_597.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(H_597,\"H_597.graphml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
